---
title: "ST 558 Project 3"
author: "Melanie Kahn & Rachel Hardy"
date: "2022-11-12"
output: 
  html_document:  
    toc: true
    toc_depth: 3
    toc_float: true
    theme: readable
    df_print: tibble
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
```

# Required Libraries

Running the code chunk below loads the `tidyverse`, `readr`, `ggplot2`, `corrplot`, and `caret` packages.

```{r}
library(tidyverse)
library(readr)
library(ggplot2)
library(corrplot)
library(caret)
```

# Introduction to the Data

The online news popularity data used for this project summarizes a diverse set of features about articles published by [Mashable](http://www.mashable.com) over a two year period with the goal of predicting the number of shares in social networks - a proxy for popularity.

The original online news popularity data set included 58 predictive variables, 2 non-predictive variables, 1 target variable. For the purposes of this project, we are only using a subset of this original data to contain _ predictive variables and _ non-predictive variables, keeping the same target variable.

The variables present for each observation in this subset of the online news popularity data set are as follows:

Non-Predictive Variables:

* `url` - URL of the article  
* `timedelta` - The number of days between the article publication and the data set acquisition  

Predictive Variables:

* `data_channel_is_*` - Binary variable indicating the type of data channel  
    + `lifestyle` - Lifestyle  
    + `entertainment` - Entertainment  
    + `bus` - Business  
    + `socmed` - Social Media  
    + `tech` - Tech  
    + `World` - World  
* `is_weekend` - Binary variable indicating if the article published on the weekend  
* `weekday` - What day of the week the article was published (factor variable with seven levels)  
* `num_imgs` - The number of images in the article  
* `num_keywords` - The number of keywords in the metadata  
* `n_tokens_title` - The number of words in the title  
* `title_subjectivity` - Score of 0 - 1 indicating how subjective the title of the article is  
* `global_subjectivity` - Score of 0 - 1 indicating how subjective the text of the article is  

Target Variable:

* `shares` - Number of shares  

# Data

## Reading in the Data

Running the code chunk below reads in the online news popularity data set using `read_csv()`.

```{r}
newsOriginal <- read_csv(file = "./OnlineNewsPopularity.csv")
newsOriginal
```

## Modifying the Data

Running the code chunk below subsets the data to only include observations for the data channel we're interested in, `data_channel_is_socmed`. All other `data_channel_is_*` observations have been removed from the data set.

```{r}
news <- newsOriginal %>% filter(data_channel_is_socmed == 1)
news
```

Running the code chunk below creates the categorical variable `weekday` to the data set that tells us what day of the week the article was published.

```{r}
news <- news %>% mutate(weekday = if_else((weekday_is_monday == 1), "Monday",
                                  if_else((weekday_is_tuesday == 1), "Tuesday", 
                                  if_else((weekday_is_wednesday == 1), "Wednesday", 
                                  if_else((weekday_is_thursday == 1), "Thursday", 
                                  if_else((weekday_is_friday == 1), "Friday",
                                  if_else((weekday_is_saturday == 1), "Saturday", 
                                  if_else((weekday_is_sunday == 1), "Sunday", " ")))))))) %>%
                 select(url, shares, weekday, everything())

news$weekday <- factor(news$weekday, levels=c("Monday", "Tuesday", "Wednesday",
                                              "Thursday", "Friday", "Saturday", "Sunday"))
levels(news$weekday)

news
```

## Splitting the Data

Running the code chunk below splits the modified `news` data set into a training and testing set using `createDataPartition()`. First the seed is set to make sure the random sampling will be reproducible. `createDataPartition()` then creates an indexing vector (`trainIndex`) with a subset of the `shares` variable where the training subset (`newsTrain`) will result in a vector (`list = FALSE`) that has approximately 70% (`p = 0.7`) of the observations from the updated `news` data set. This training vector is then used to create the training set (`newsTrain`) with approximately 70% of the observations from the updated `news` data set, and the test set (`newsTest`) with the remaining 30% of the observations.

```{r}
set.seed(100)
newsIndex <- createDataPartition(news$shares, p = 0.7, list = FALSE)

newsTrain <- news[newsIndex, ]
newsTest <- news[-newsIndex, ]

newsTrain
newsTest
```

# Summarizations

## Summary Statistics

Running the code chunk below provides the mean and standard deviation for the number of times articles in the `news` data set were shared (`shares`).

```{r}
mean(news$shares)
sd(news$shares)
```

Running the code chunk below provides the mean and standard deviation for the number of images per article (`num_imgs`) in the `news` data set.

```{r}
mean(news$num_imgs)
sd(news$num_imgs)
```

Running the code chunk below provides the mean and standard deviation for the number of keywords per article (`num_keywords`) in the `news` data set.

```{r}
mean(news$num_keywords)
sd(news$num_keywords)
```

## Contingency Tables

Running the code chunk below creates a contingency table showing the number of articles in the online `news` popularity data set that were published on the weekend (`is_weekend`).

```{r}
tableWeekend <- table(news$is_weekend)
tableWeekend
```

From the table **above**, we can see that `r tableWeekend[2]` articles were published on the weekend, and `r tableWeekend[1]` articles were published during the week.

Running the code chunk below creates a contingency table showing the number of articles in the online `news` popularity data set that were published on certain days of the week (`weekday`).

```{r}
tableWeekday <- table(news$weekday)
tableWeekday
```

## Graphical Summaries

### Bar Plot

Running the code chunk below creates a bar plot to visualize the number of articles published per each `weekday`. Using the aesthetics option `aes(fill = weekday)` inside the `geom_bar()` function gives us a nicely colored graph.

```{r}
g <- ggplot(news, aes(x = weekday))

g + geom_bar(aes(fill = weekday)) + 
  labs(title = "Number of Articles Published by Weekday", x = "Weekday") +
  scale_fill_discrete(name = "Weekday")
```

### Box Plot 

Running the code chunk below creates a box plot of number of `shares` for each `weekday`. Using the aesthetics option 'fill = weekday' gives us a nicely colored graph.

```{r}
g <- ggplot(news, aes(x = weekday, y = shares))

g + geom_boxplot(aes(fill = weekday)) + 
  labs(title = "Box Plot of Shares by Weekday", x = "Weekday", y = "Shares") +
  scale_fill_discrete(name = "Weekday")
```

### Histograms

Running the code chunk below creates two histograms of the number of `shares` that show us the distribution of the variable. The second histogram has an added density layer to give us a better idea of how the data is spread out.

```{r}
g <- ggplot(news, aes(x = shares))

g + geom_histogram(color = "black", fill = "#FF6666") + labs(title = "Histogram of Shares") +
  labs(title = "Histogram of Shares", x = "Shares")

g + geom_histogram(aes(y=..density..), colour="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + 
  labs(title = "Histogram of Shares with Density", x = "Shares")
```

### Scatter Plots

Running the code chunk below creates a scatter plot to visualize the correlation between the text subjectivity (`global_subjectivity`) and the number of images (`num_imgs`) articles have. The `geom_point()` function plots the data points while the `geom_smooth()` function plots the regression line using method `lm` for linear model.

The results show a weak, positive linear correlation between an article's overall subjectivity and the number of images used. It is an interesting finding that subjectivity would increase with the number of images, even if only slightly.

```{r}
g <- ggplot(news, aes(x = global_subjectivity, y = num_imgs))
g + geom_point() +
  geom_smooth(method = lm, col = "Blue") +
  labs(title = "Relationship Between Text Subjectivity and Number of Images",
       x = "Text Subjectivity",
       y = "Number of Images")
```

Running the code chunk below creates a scatter plot to visualize the correlation between the number of `shares` and the number of keywords (`num_keywords`) articles have. `geom_jitter` is used instead of `geom_point()` to plot the data points in a manner where the `weekday` component can be better visualized. The `geom_smooth()` function plots the regression line using method `lm` for linear model.

The results show a weak, positive linear correlation between the number of shares and keywords, meaning that as the number of keywords increases, so does the popularity of the article. As of of the default arguments for the `geom_smooth` function is `se = TRUE`, we can also see that the confidence interval widens as shares increase, indicating that uncertainty of the effect is greater.

```{r}
g <- ggplot(news, aes(x = shares, y = num_keywords))
g + geom_jitter(aes(color = weekday)) +
  geom_smooth(method = lm, col = "Blue") +
  labs(title = "Relationship Between Popularity and Number of Keywords",
       x = "Shares",
       y = "Number of Keywords")
```

Running the code chunk below creates a facet grid scatter plot to visualize the correlation between the number of words in the article's title (`n_tokens_title`) and title's subjectivity score (`title_subjectivity`) according to the day the article was published (`weekday`).  The `geom_point()` function plots the data points while the `geom_smooth()` function plots the regression line using method `lm` for linear model. 

The results show a weak, positive linear correlation between title subjectivity and length on Mondays, Fridays and Sundays. Tuesday and Wednesday were also showed slightly positive, but generally speaking there is little to no correlation. Thursday and Saturday showed weak, negative correlations. All of these weak correlations tell us that the number of words in an article's title has little to no impact on the title's overall subjectivity rating.

```{r}
g <- ggplot(news, aes(x = title_subjectivity, y = n_tokens_title))
g + geom_point(aes(color = weekday)) +
  facet_grid(~ weekday) +
  geom_smooth(method = lm, col = "Blue") +
  labs(title = "Relationship Between Title Subjectivity and Length",
       x = "Title Subjectivity",
       y = "Number of Words in Title")
```

# Modeling

## Linear Regression

Linear regression attempts to model the (linear) relationship between a response variable and one or more predictor variables by fitting a linear equation to the data. The simplest form of the linear equation is `Y = a + bX`, where `Y` is the response variable, `a` is the intercept, `b` is the slope, and `X` is the predictor (or explanatory) variable. The most common method for fitting a regression model is least-squares regression, where the best-fitting line is calculated by minimizing the sum of the squared residuals.

For linear regression, it is usually important to understand which variables are related and which variables scientifically should be in the model. It is also important to split the data into a training set and a testing set so the model does not become over-fit.

Running the code chunk below creates a multiple linear regression model where `shares` is the response variable and the predictor variables are `weekday`, `title_subjectivity`, `num_imgs`, `title_subjectivity^2`, and `num_imgs^2`.

By using the `summary()` function, we can see the values for the residuals and coefficients, as well as the performance criteria values such as multiple R-squared.

```{r}
set.seed(100)
firstLinearModel <- train(shares ~ weekday + title_subjectivity + num_imgs + I(title_subjectivity^2) + I(num_imgs^2), 
                        data = newsTrain,
                        method = "lm",
                        preProcess = c("center", "scale"),
                        trControl = trainControl(method = "cv"))
firstLinearModel

summary(firstLinearModel)

firstLinearRMSE <- firstLinearModel$results$RMSE
firstLinearRMSE
```

Running the code chunk below creates a simple linear regression model where `shares` is the response variable and the predictor variables are `weekday`, `num_imgs`, `num_keywords`, `n_tokens_title`, `title_subjectivity`, and `global_subjectivity`. The `summary()` function is used to examine the values for the residuals and coefficients, as well as the performance criteria values such as multiple R-squared.

```{r}
set.seed(100)
secondLinearModel <- train(shares ~ weekday + num_imgs + num_keywords + n_tokens_title + title_subjectivity + global_subjectivity, 
                        data = newsTrain,
                        method = "lm",
                        preProcess = c("center", "scale"),
                        trControl = trainControl(method = "cv"))
secondLinearModel

summary(secondLinearModel)

secondLinearRMSE <- secondLinearModel$results$RMSE
secondLinearRMSE
```

## Random Forest

To understand random forests, it is first important to understand bagged trees which are created using bootstrap aggregation. For bagged trees, the sample is treated as the population and re-sampling is done with replacement. The process of creating a bagged tree is below:

* Step 1: Create a bootstrap sample using `sample()`  
* Step 2: Train the tree on this sample (no pruning necessary)  
* Step 3: Repeat B = 1000 times (no set mark)  
* Step 4: Final prediction is average of these predictions (for regression trees) **OR** use majority vote as final classification prediction (classification trees)  

Random forests are essentially bagged trees, except not all the predictors are used for each model. A random subset of predictors is used for each tree model (bootstrap sample). The purpose of doing this is to prevent one or two strong predictors from dominating all tree models and creating unwanted correlation between models.

Running the code chunk below trains the random forest model. The formula notation used in the `train()` function models the `shares` variable using the following predictor/explanatory variables: `weekday`, `num_imgs`, and `num_keywords`. To use the random forest model, the `method` argument was specified as `"rf"`. The data was pre-processed by centering and scaling. Cross validation was used five-fold and repeated three (3) times. The argument `tuneGrid` was then used to replicate the random forest model a total of five (5) times. The best model is then chosen based on the performance criteria.

```{r}
set.seed(100)
randomForestCtrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
randomForestFit <- train(shares ~ weekday + num_imgs + num_keywords, 
                         data = newsTrain, method = "rf", 
                         trControl = randomForestCtrl,
                         preProcess = c("center","scale"), 
                         tuneGrid = data.frame(mtry = 1:5))

randomForestFit
```


```{r}
randomForestPredict <- predict(randomForestFit, newdata = newsTest)

randomForestPerformance <- postResample(randomForestPredict, newsTest$shares)
randomForestPerformance

attributes(randomForestPerformance)

rfRMSE <- randomForestPerformance[1]
rfRMSE
```

## Boosted Tree

Boosted trees are another enhancement to the single tree methods. However, unlike bagged and random forest models, boosted trees do not use bootstrapping. Boosting is a general method to slowly train your tree so you don't overfit your model. The trees are grown in a sequential manner where each subsequent tree is based off a modified version of the original data, updating the predictions as the tree is grown. The process is described below:


* Step 1: Initialize predictions as 0  
* Step 2: Find the residuals for every observation  
    + Residuals in first tree fit will be original data values (observed - 0 = observed)  
* Step 3: Fit a regression tree with `d` splits where the residuals are the response  
* Step 4: Update predictions using the new predictions from step 3 multiplied by the growth rate (Lambda tuning parameter)  
* Step 5: Continue to update residuals for new predictions (steps 2 -4) `B` times  

Running the code chunk below trains the boosted tree model. The formula notation used in the `train()` function models the `shares` variable using the following predictor/explanatory variables: `weekday`, `num_imgs`, `num_keywords`, `n_tokens_title`, and `title_subjectivity`. To use the boosted tree model, the `method` argument was specified as `"gbm"`. The data was pre-processed by centering and scaling. `tuneGrid` was then used to consider values of `n.trees` = 50, `interaction.depth` = 1, `shrinkage` = 0.1, and `n.minobsinnode` = 10. Lastly, `trainControl()` was used within the `trControl` argument to do 10 fold cross-validation using the `"cv"` `method`.

```{r}
boostTreeFit <- train(shares ~ weekday + num_imgs + num_keywords + n_tokens_title + title_subjectivity
                        + global_subjectivity, data = newsTrain,
                        method = "gbm",
                        preProcess = c("center", "scale"),
                        tuneGrid = data.frame(n.trees = 50, interaction.depth = 1, shrinkage = 0.1, n.minobsinnode = 10),
                        trControl = trainControl(method = "cv", number = 10))
boostTreeFit
```

Now that the boosted tree model has been trained (`boostTreeFit`), running the code chunk below will check how well the model does on the test set `newsTest` using the `postResample()` function. 

```{r}
boostingPredict <- predict(boostTreeFit, newdata = newsTest)

boostTreePerformance <- postResample(boostingPredict, newsTest$shares)
boostTreePerformance

attributes(boostTreePerformance)

boostRMSE <- boostTreePerformance[1]
boostRMSE
```

# Comparison

For this section, we will write a function called `bestModel()` to compare all of the RMSE values for the four models above and choose the best one (i.e. the smallest one). 

```{r}
#This one works but it doesn't say which model is the one with the lowest value.

bestModel <- function(linear1, linear2, rf, boost){
  vec <- c(linear1, linear2, rf, boost)
  bestRMSE <- min(vec)
  
  return(bestRMSE)
}

bestModel(firstLinearRMSE, secondLinearRMSE, rfRMSE, boostRMSE)
```

```{r}
#Trying another one here!

bestModel2 <- function(linear1, linear2, rf, boost){
  #Function stuff here.
}
```

# Automation


